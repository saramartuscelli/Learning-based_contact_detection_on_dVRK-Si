{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uidb8rsUPR_w"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "4jca-AINqpbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/1.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/1.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/3.csv')\n",
        "df4 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/4.csv')\n",
        "df5 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/5.csv')\n",
        "df6 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/6.csv')\n",
        "df7 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/7.csv')\n",
        "df8 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/8.csv')\n",
        "df9 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/9.csv')\n",
        "df10 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/10.csv')\n",
        "\n",
        "df_validation = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/validation.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/THESIS/data_oct9/test.csv')"
      ],
      "metadata": {
        "id": "gyc1yVRlqmVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n"
      ],
      "metadata": {
        "id": "ksSifjiVhUFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# outliers removal\n",
        "\n",
        "df4 = df4.drop(range(3500, 5500))\n",
        "df6 = df6.drop(range(47000, len(df6)))\n",
        "df9 = df9[(df9['contacts'])==0]"
      ],
      "metadata": {
        "id": "VyWZ9d5qvsZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting\n"
      ],
      "metadata": {
        "id": "XiokYLFQP0Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10], ignore_index=True)\n",
        "\n",
        "print(\"Training test:\", df_train.shape)\n",
        "print(\"Validation test:\", df_validation.shape)\n",
        "print(\"Test test:\", df_test.shape)"
      ],
      "metadata": {
        "id": "cN7d8UCKP0Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ],
      "metadata": {
        "id": "CzdXQF5eGG_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.plot(subplots=True, figsize=(50, 50))"
      ],
      "metadata": {
        "id": "3JDq8fZ4P_pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation.plot(subplots=True, figsize=(50, 50))"
      ],
      "metadata": {
        "id": "ktPW6L14QMzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.plot(subplots=True, figsize=(50, 50))"
      ],
      "metadata": {
        "id": "-M1hfgeDQMrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Inputs (X and y)"
      ],
      "metadata": {
        "id": "biuFeCqmQWAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train.drop(['timestamps_sensor', 'contacts'], axis=1)\n",
        "y_train = df_train['contacts']\n",
        "print(\"Training: \", X_train.shape)\n",
        "\n",
        "X_validation = df_validation.drop(['timestamps_sensor', 'contacts'], axis=1)\n",
        "y_validation = df_validation['contacts']\n",
        "print(\"Validation: \", X_validation.shape)\n",
        "\n",
        "X_test = df_test.drop(['timestamps_sensor', 'contacts'], axis=1)\n",
        "y_test = df_test['contacts']\n",
        "print(\"Test: \", X_test.shape)"
      ],
      "metadata": {
        "id": "bmGHwBdrQVjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Scaling"
      ],
      "metadata": {
        "id": "vpOYRqqIQat6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale data between -1 and 1\n",
        "import numpy as np\n",
        "\n",
        "max_train = X_train.max()\n",
        "min_train = X_train.min()\n",
        "\n",
        "print(\"max_train = \", list(max_train.values))\n",
        "print(\"min_train = \", list(min_train.values), \"\\n\")\n",
        "\n",
        "X_train_scaled = (X_train - min_train) / (max_train - min_train) * 2 - 1\n",
        "X_validation_scaled = (X_validation - min_train) / (max_train - min_train) * 2 - 1\n",
        "X_test_scaled = (X_test - min_train) / (max_train - min_train) * 2 - 1\n",
        "\n",
        "print(\"Training: \", X_train_scaled.shape)\n",
        "print(\"Training: \", X_validation_scaled.shape)\n",
        "print(\"Test: \", X_test_scaled.shape)"
      ],
      "metadata": {
        "id": "DrEMygnlQdYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sliding Windows\n"
      ],
      "metadata": {
        "id": "0HHkU8NJtJBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Generator (Train e Validation)"
      ],
      "metadata": {
        "id": "I4G8mNvHDaLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def window_generator(data, labels, window_size=100, step=10, strategy='center'):\n",
        "    n_samples = len(data)\n",
        "    for i in range(0, n_samples - window_size + 1, step):\n",
        "        window = data[i:i+window_size]\n",
        "        window_labels = labels[i:i+window_size]\n",
        "\n",
        "        # label definition with different strategies\n",
        "        if strategy == 'center':\n",
        "            label = window_labels[window_size // 2]\n",
        "        elif strategy == 'max':\n",
        "            label = np.max(window_labels)\n",
        "        elif strategy == 'mode':\n",
        "            label = np.bincount(window_labels).argmax()\n",
        "        elif strategy == 'probability':\n",
        "            label = np.mean(window_labels)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
        "\n",
        "        yield window.astype(np.float32), np.float32(label)    # yield returns one window at time -> should not saturate RAM"
      ],
      "metadata": {
        "id": "vOLN_dPa2Lcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "window_size = 100\n",
        "step = 10\n",
        "strategy = 'probability'\n",
        "batch_size = 64\n",
        "\n",
        "train_dim = (len(X_train_scaled) - window_size + 1) // step\n",
        "validation_dim = (len(X_validation_scaled) - window_size + 1) // step\n",
        "\n",
        "train_windows = tf.data.Dataset.from_generator(\n",
        "    lambda: window_generator(X_train_scaled, y_train, window_size, step, strategy),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(window_size, X_train_scaled.shape[1]), dtype=tf.float32),  # X\n",
        "        tf.TensorSpec(shape=(), dtype=tf.float32)                                       # y\n",
        "    )\n",
        ")\n",
        "\n",
        "train_windows = train_windows.shuffle(buffer_size=train_dim, reshuffle_each_iteration=True)\n",
        "train_windows = train_windows.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "validation_windows = tf.data.Dataset.from_generator(\n",
        "    lambda: window_generator(X_validation_scaled, y_validation, window_size, step, strategy),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(window_size, X_validation_scaled.shape[1]), dtype=tf.float32),  # X\n",
        "        tf.TensorSpec(shape=(), dtype=tf.float32)                                            # y\n",
        "    )\n",
        ")\n",
        "\n",
        "# no shuffle nel validation set!\n",
        "validation_windows = validation_windows.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "P_EsUD2R2j7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Dataset (Test)"
      ],
      "metadata": {
        "id": "ZUqBKFwgDUOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_windows(data, labels, window_size=100, step=10, strategy='center'):\n",
        "\n",
        "    X, y, idx = [], [], []\n",
        "    n_samples, n_features = data.shape\n",
        "\n",
        "    for i in range(0, n_samples - window_size + 1, step):\n",
        "        window = data[i:i+window_size]\n",
        "        window_labels = labels[i:i+window_size]\n",
        "        index = i + window_size // 2\n",
        "\n",
        "        if strategy == 'center':\n",
        "            label = window_labels[window_size // 2]\n",
        "        elif strategy == 'max':\n",
        "            label = np.max(window_labels)\n",
        "        elif strategy == 'mode':\n",
        "            label = np.bincount(window_labels).argmax()\n",
        "        elif strategy == 'probability':\n",
        "            label = np.mean(window_labels)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
        "\n",
        "        X.append(window)\n",
        "        y.append(label)\n",
        "        idx.append(index)\n",
        "\n",
        "    return (\n",
        "        np.array(X, dtype=np.float32),\n",
        "        np.array(y, dtype=np.float32),\n",
        "        np.array(idx, dtype=np.int64)\n",
        "    )"
      ],
      "metadata": {
        "id": "svjW9TMTDThX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_windows, y_test_windows, index_test = create_windows(X_test_scaled.values, y_test.values,\n",
        "                                                            window_size=window_size,\n",
        "                                                            step=step,\n",
        "                                                            strategy=strategy)\n",
        "\n",
        "print(\"Test test:\", X_test_windows.shape, y_test_windows.shape)"
      ],
      "metadata": {
        "id": "Kz1m2vLcQz2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "xy4Q4CQgQ1pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear keras from previous session\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "DH3ctIesQ4bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.metrics import Recall, Precision, AUC\n",
        "\n",
        "time_steps = X_test_windows.shape[1]    # window size\n",
        "n_features = X_test_windows.shape[2]    # number of features per sample\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(time_steps, n_features)),\n",
        "\n",
        "    # LSTM\n",
        "    LSTM(128, return_sequences=False, dropout=0.3),\n",
        "\n",
        "    # Final dense\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['accuracy', Recall(), Precision()]\n",
        ")\n",
        "\n",
        "print(\"Input dimensions: \", model.input_shape)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gAmLcefqQ4-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Early stop to avoid overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Save best model\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/Colab Notebooks/THESIS/Saved Models oct12/best_model_.keras\",\n",
        "                             monitor='val_loss',\n",
        "                             save_best_only=True,\n",
        "                             mode='min')"
      ],
      "metadata": {
        "id": "2k6Yqg9cRDPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "cmgMnDH1RFoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "history = model.fit(\n",
        "    train_windows,\n",
        "    validation_data = validation_windows,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop, checkpoint],\n",
        "    steps_per_epoch = math.ceil(train_dim / batch_size),\n",
        "    validation_steps = math.ceil(validation_dim / batch_size)\n",
        ")"
      ],
      "metadata": {
        "id": "AN3KMkIwRHk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "GH08TqLmRQcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "thr = 0.5\n",
        "\n",
        "y_pred_prob = saved_model.predict(X_test_windows).flatten()\n",
        "y_pred = (y_pred_prob > thr).astype(int)\n",
        "y_test = (y_test_windows > thr).astype(int)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"CONFUSION MATRIX:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "B9KV-hlkRQL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "ODU3AlY_v-AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(50, 30))\n",
        "\n",
        "plt.subplot(311)\n",
        "plt.plot(df_test['acc_x'], color='blue', alpha=0.3)\n",
        "plt.plot(index_test, y_test_windows, color='gray', label='true')\n",
        "plt.plot(index_test, y_pred_prob, color='black', label='pred')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(312)\n",
        "plt.plot(df_test['acc_y'], color='red', alpha=0.3)\n",
        "plt.plot(index_test, y_test_windows, color='gray', label='true')\n",
        "plt.plot(index_test, y_pred_prob, color='black', label='pred')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(313)\n",
        "plt.plot(df_test['acc_z'], color='green', alpha=0.3)\n",
        "plt.plot(index_test, y_test_windows, color='gray', label='true')\n",
        "plt.plot(index_test, y_pred_prob, color='black', label='pred')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9p6P2yTnRf-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX Convertion"
      ],
      "metadata": {
        "id": "_tmAu5qkRkqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"numpy<2.0\""
      ],
      "metadata": {
        "id": "MwmAu3k2Rp67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf2onnx onnx"
      ],
      "metadata": {
        "id": "xwJE5aGtRtQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tf2onnx\n",
        "\n",
        "# force the use of the CPU\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    model = tf.keras.models.load_model(\n",
        "        \"/content/drive/MyDrive/Colab Notebooks/THESIS/Saved Models oct12/best_model_1.keras\",\n",
        "        compile=False\n",
        "    )\n",
        "\n",
        "model_func = tf.keras.Model(inputs=model.inputs, outputs=model.outputs)\n",
        "\n",
        "time_steps = 100\n",
        "n_features = 19\n",
        "spec = (tf.TensorSpec((None, time_steps, n_features), tf.float32, name=\"input\"),)\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/THESIS/Saved Models oct12/best_model_1_cpu.onnx\"\n",
        "model_proto, _ = tf2onnx.convert.from_keras(\n",
        "    model_func,\n",
        "    input_signature=spec,\n",
        "    output_path=output_path,\n",
        "    opset=13\n",
        ")\n",
        "\n",
        "print(\"Model converted to ONNX format (CPU-compatibile) and saved in:\", output_path)"
      ],
      "metadata": {
        "id": "dYY3qxc7SF7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}